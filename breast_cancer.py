# -*- coding: utf-8 -*-
"""Breast Cancer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HxL32ots2Zn1mxVO3yGzTzL4qTw1WLD1
"""

import os
import pandas as pd
import numpy as np
import tensorflow as tf
import matplotlib.pylab as plt
import glob

_URL = 'https://storage.googleapis.com/kaggle-data-sets/7415/10564/bundle/archive.zip?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=gcp-kaggle-com%40kaggle-161607.iam.gserviceaccount.com%2F20211118%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20211118T121352Z&X-Goog-Expires=259199&X-Goog-SignedHeaders=host&X-Goog-Signature=5f3535bc15d84553601970f1b1accefce44450f75f74f04b6187912364757d958b9c48330c9a1eab1bfada97e0f5a28db24ffe1fa04020da05a5ec1224a04ca420a71c8383104b983a9748c587cc48868d9668713e507cca68ff3352df573b630a4ab704c85bec7f70f38315c8c66598a2c25cb465575aad3a954b5a01507a65098acb07fd22a27f109c357ea0750a4ef012ca2122a6391c435f1243e0ee3ffe5687b8f09cfdc3c154ffbf3c7b6fd74b736ed47b9d62dd1815f4e2331def941960a821c53bc632ae39708c4eb3f0c4b84efc67a640c39b3d7c3d0cb5893c2fa72da5661a3076b92bc9288e008263e3eb280915bfab2df7e864fbdcb4e76a5ae2'

zip_dir = tf.keras.utils.get_file('archieve.zip', origin=_URL, extract=True)

base = '/root/.keras/datasets/IDC_regular_ps50_idx5/'
from tqdm.notebook import tqdm
ids = os.listdir(base)
data = []
for id in tqdm(ids):
  try:
    files1 = os.listdir(base + id + '/1/')
    files0 = os.listdir(base + id + '/0/')
    for x in files1:
      data.append(base + id + '/1/' + x)
    for x in files0:
      data.append(base + id + '/0/' + x)
  except:
    FileNotFoundError
len(data)

class0 = [] # 0 = Images of no cancer
class1 = [] # 1 = Images with cancer

for filename in data:
    if filename.endswith("class0.png"):
         class0.append(filename)
    else:
        class1.append(filename)

len(class0)

len(class1)

import random

random.shuffle(data)
data = data[:20000]
len(data)

import keras_preprocessing.image as IMAGE
from PIL import Image

images=[]
labels=[]

for i in tqdm(data):
  label = int(i[-5])
  img = IMAGE.img_to_array(IMAGE.load_img(i, target_size=(50, 50)))
  images.append(img)
  labels.append(label)

from sklearn.model_selection import train_test_split

y = np.array(labels)
x = np.stack(images)/255

x_train,x_test,y_train,y_test = train_test_split(x, y, random_state=0, test_size=0.3)
print(x_train.shape, x_test.shape, y_train.shape, y_test.shape)

from tensorflow.keras.utils import to_categorical

from tensorflow.keras import Sequential
from tensorflow.keras.optimizers import SGD, RMSprop, Adam, Adagrad, Adadelta
from keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization, Conv2D, MaxPool2D, MaxPooling2D

model=Sequential()

model.add(Conv2D(filters=32,kernel_size=(4,4),input_shape=(50,50,3),activation='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.3))
model.add(Conv2D(filters=32,kernel_size=(4,4),activation='relu'))
model.add(MaxPool2D(pool_size=(2,2)))
model.add(Dropout(0.25))
model.add(Flatten())
model.add(Dense(64,activation='relu'))
model.add(Dense(1,activation='sigmoid'))
model.compile(loss = 'binary_crossentropy', optimizer ='adam', metrics= ['accuracy'])

model.summary()

history = model.fit(x_train, 
                    y_train, 
                    validation_data = (x_test, y_test), 
                    epochs = 100, 
                    verbose = 2, 
                    batch_size = 256)

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""**This Graphs shows that after 20th Epoch model starts overfitting and thats why model should be trained only till 20th**"""